---

settings:

  # Where to get the data
  cifar: &cifar
    url: "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
    checksum: "6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce"
    path: "~/kur"

  # Backend to use
  backend:
    name: keras
    # backend: tensorflow

  # Hyperparameters
  cnn:
    kernels: [64, 32]
    size: [2, 2]
    strides: [1, 1]

# The model itself.
# This is parsed immediately after the "parameters" block.
model:
  - input: images
  - for:
      range: "{{ cnn.kernels|length }}"
      iterate:
        - convolution:
            kernels: "{{ cnn.kernels[index] }}"
            size: "{{ cnn.size }}"
            strides: "{{ cnn.strides }}"
        - activation:
            # name: relu
            name: leakyrelu # leakyrelu # relu
            alpha: 0.7 # if alpha not exist or empty as None, default value is 0.3
  - flatten:
  - dense: 10
  - activation:
      name: softmax
    name: labels

train:
  data:
    - cifar:
        <<: *cifar
        parts: [1, 2, 3, 4]
  provider:
    batch_size: 32
    num_batches: 1
  log: cifar-log
  epochs:
    number: 2
    mode: additional
  stop_when:
    epochs: 1 # null or infinite : to train forever
    elapsed:
      minutes: 10
      hours: 0
      days: 0
      clock: all # (time spend on all things) or all | train | validate | batch
    mode: additional # additional | total, if set total, then elapsed above define total training time in history added
  hooks:
    - plot_weights:
        plot_every_n_epochs: 1
        plot_directory: "plot_weights2"
        weight_file: cifar.best.valid.w
        weight_keywords1: ["convolution.0", "kernel"]
        weight_keywords2: ["convolution.1", "kernel"]
    - plot: # the folder must be prepared first
        loss_per_batch: cifar-plot/plot1.png
        loss_per_time: cifar-plot/plot2.png
        throughput_per_time: cifar-plot/plot3.png
  weights: # the folders below are prepared automatically?
    initial: cifar.best.valid.w
    best: cifar.best.train.w
    last: cifar.last.w
  checkpoint:
    path: cifar-checkpoint/
    batches: 500 # batches, samples, epochs, minutes if present, must be an integer, not a string, not null, not None
    samples: 1000
    epochs: 1
    minutes: 1000
    validation: no
  optimizer:
    name: adam
    learning_rate: 0.001

validate:
  data:
    - cifar:
       <<: *cifar
       parts: 5
  provider:
    num_batches: 1
  weights: cifar.best.valid.w
  hooks:
    - output: # folder and file must be prepared first
        path: cifar-output-validate/output.pkl
        format: pickle


test: &test
  data:
    - cifar:
       <<: *cifar
       parts: test
  weights: cifar.best.valid.w
  provider:
    num_batches: 10

evaluate:
  <<: *test
  destination: cifar.results.pkl
  hooks:
    - output:
        path: cifar-output-hook/output.pkl
        format: pickle

loss:
  - target: labels
    name: categorical_crossentropy
...
