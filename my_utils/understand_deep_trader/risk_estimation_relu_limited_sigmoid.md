# 本次讨论实现的结果
risk_estimation_relu_limited_sigmoid


文档完成时间：2017.7.12-16
- 深入认知risk_estimation损失函数的设计原理
- 深入对比relu_limited的特点，对比sigmoid函数

## 优化该损失函数的目的
减少交易频率
让损失值降得更低

## 该损失函数的定义
```python
def risk_estimation(y_true, y_pred):
    return -100. * K.mean((y_true - 0.0002) * y_pred)
```

## 概念诠释
- `y_true`: 当日收盘价与昨日收盘价变化比， 常规区间（-0.1， 0.1），历史中会有少数极端情况存在
- `y_pred`: 当日（当日开盘后要最新的，不再是昨日收盘前更新的，这样更方便理解）持仓市值在总资产中的占比， 区间（0，1）
- 交易成本： 0.0002
- K.mean: 取均值
- K.mean((y_true - 0.0002) * y_pred) ：这部分的内涵是，平均每日总资产的变化比，在忽略交易成本情况下，常规变化区间（-0.1， 0.1）
- 学习算法的目的，是通过训练更新参数，让损失函数的损失值是趋近于最小值，这里是-0.1
- 但我们希望‘平均每日总资产的变化比‘这个值是越大越好，最好永远是0.1，所以要加一个“负号-”在前面
- -100. * K.mean((y_true - 0.0002) * y_pred) ： 这里加上了“-”负号
- 但为什么要乘100？我猜测是为了让损失值不要太小，看起来不舒服（乘不乘100其实不重要）对吗？@happynoom??
	- 回复：是的
- 这样一来，损失值越小，平均每日总资产的变化比，这个值就越大，这是我们想要的，也满足了损失函数的定义。

## 导致交易频率高的可能原因
### 猜测
- 会不会是，损失函数的设计并没有有效地将真实交易成本的影响包含进去？
	- 回复：是的持仓变动会导致交易成本，目前损失函数没有有效的包含着部分损失。
- 如果损失函数真实记录了交易成本的影响，也许模型会尽可能权衡从而降低交易频率？
	- 回复：是这样的
### 潜在问题
实际交易成本应该更高：0.001 比0.0002 要现实一些
实际交易成本发生条件更复杂：上述公式认为，只要有持仓市值（非0），就要发生交易成本（但现实并非如此，除非每天都全部卖出标的，在重新买入所需持仓，但这明显也不合理）
回复：过大的交易成本（例如0.001会导致模型倾向于不持仓。而0.0002是一个持仓的平均成本是所有交易均摊后的均值）
step_up： 感觉这个逻辑行得通，将0.0002作为一个交易成本均值，分摊到每一次非0的预测中
那么更现实产生交易成本的情况是怎样的？
空仓变持仓：成本=0.001*y_pred*当天开盘时总资产
持仓变空仓：成本=0.001*y_pred*当天开盘时总资产
持仓有增减：成本=0.001*|当日开盘时总资产*当日y_pred-昨日开盘时总资产*昨日y_pred|
无交易成本的情况：
当日持仓市值 ==昨日持仓市值： 当日开盘时总资产*当日y_pred ==昨日开盘时总资产*昨日y_pred
@happynoom, 有可能将这些情况写入risk_estimation吗？损失函数的设计，可以调用类似y_pred[i-1]吗？
    回复：使用y_pred[i-1]是不太可能的，交易成本的确如您所描述的那样，但是如何量化进每天的损失是个难题

建议
如果将y_pred定义为当日开盘后我在市场内的持仓市值，我们可以省去一个概念的计算：即，当日的总资产，昨日总资产
--------
MonteCarlo: 损失函数基本上搞懂了，里面内含的应该是把每天的损失当作一个独立事件，所以求平均损失最小。因此如果换成持仓市值，前后交易日是会互相影响，对历史路径有依赖，可能解出来的参数就没有普适性

step_up: 是的！目前的状况感觉很纠结：
如果，损失函数里抛弃交易成本（不予考虑），而实际交易频率又很高，这样的模型训练效果和实际交易结果很定存在较大差异；
如果，损失函数要考虑交易成本，而真实的交易成本产生情况又比较复杂，不能简单用“-0.0002”，而需要调用到类似y_pred[i-1]，y_true[i-1]的值；
但常规意义上，损失函数的设计，似乎不应该使用y_pred[i-1]，y_true[i-1]的值，正如@monteCarlo 所说，“前后交易日是会互相影响，对历史路径有依赖，可能解出来的参数就没有普适性”。这也是为什么建议在每轮（epoch)训练前都要对训练数据（training set)作随机（shuffle）的原因吧
@happynoom 你觉得以上的分析有问题吗？
happynoom：@MonteCarlo的分析是很有道理的, step_up关于目前损失函数的缺点分析也是正确的，如果解决目前我还没想好

step_up:  解决的思路似乎可以是：
不要有历史依赖-->不要用y_pred[i-1]，y_true[i-1] ---> 不要计算交易成本--->  采取其他方法（抛弃将交易成本包含到损失函数的方法）降低交易频率
这也意味：
1）在损失函数里，抛弃-0.0002，抛弃整个交易成本概念；
2）预测值的内涵，定义为当天开盘时要买入的总资金，而不在是当天开盘时要持有的总市值在总资金中的占比；因为（总市值*价格变化）要比（市值占比*价格变化）更准确的表达@happynoom的意思；
3）由于预测值内涵发生了变化，预测值的处理函数或激励函数relu_limited，是否也需要作出一定的调整呢？
预测值作为市值或总投入资金，想象的真实数值应该很大，如果还可以反手卖出，那区间应该是很小负数和很大的证书
如果沿用relu_limited，所有负值归0，所有大于1的值归1，把它用于市值或总投入资金这个概念上，感觉不合理；relu_limited处理过后预测值的分布图，如下图（以下所有图对应的预测值数据，都是基于目前最佳模型，对上证指数作出的，近700日的预测值数据）

如果不使用relu_limited来处理预测值，预测值范围似乎在(-23, 25)之间，预测值的分布图如下；

如果在去除更前一层BatchRenormalization的预测值处理，预测值范围似乎(-2, 1.4)，（是的，模型对预测值，一共做了两次处理，先被BatchRenormalization处理，再被relu_limited处理），完全没有被处理的预测值的分布图如下：

如果想让预测值区间在（0，1）之间，我们可以对预测层加上K.sigmoid处理，得到预测值的分布如下图

预测值的曲线图，如下图

降低交易频率的可能方案1：
用sigmoid来代替（BatchRenormalization和relu_limited）双处理
为处理后的预测值，选定一个阀值作为买入持有，选定另一个阀值作为卖出空仓
这个方法，逻辑上说的通吗？是否值得尝试？有别的方法吗？
    happynoom:
将激活函数改为sigmoid我有时间会尝试，
但用sigmoid代替relu_limited并不能解决频繁交易的问题，因为一旦使用阈值来截断，分布便和relu_limited没有什么不同了

step_up:
- relu_limited大幅修改了原本预测值的分布，如上图；
- 使用sigmoid可以让分布区间保持在（0，1），而且分布形态与原始预测值分布形态保持一致，这第二版模型中是如此， 见上图；
- 但如果预测值区间发生变化，不再是（-2，1.4）这样的小区间，那么处理后的预测值分布形态就无法保持原本的分布形态不变，见下图
- 下图中dense1_out是未做任何处理的预测值分布，区间较大（-20，20）；activation_1_out是做了sigmoid处理后的预测值分布，分布形态无法与原始预测值分布形态保持一致了，变成了相反的分布形态

这种变化是由sigmoid函数自身特征造成的

一种设置阀值的方法：
- 使用阀值，不是为了重新截取一个特定区间来重新分布预测值（relu_limited所做的），而是在分布不变的情况下，做人为设置买入卖出点
- 例如，买入持有阀值为0.7的目的是，在第一个大于0.7的预测值出现是买入持有；卖出空仓阀值为0.2的目的是，在第一个小于0.2的预测值出现时卖出空仓
- 从而降低交易频率，不知道是否可行？

降低交易频率的可能方案2：
- 目前特征值的维度是（？，30， 61），如果将30天拉长，比如（？， 90， 61），这个方向的尝试是否具有降低交易频率的逻辑基础呢？
- @happynoom 是否已经做过相关实验？

happynoom：    
延长天数的试验我已经做过，更长的序列意味着更深的网络，和更困难的训练，从原理上讲，更多的信息会更有用，能潜在提供更好的预测结果，但是试验中我没有得到如此的印证。我个人猜测是训练数据集不够大，不足以训练更深的网络。

step_up: 也就是说，在模型不变的情况下:
- 延长天数并没有降低交易频率是吗？
- 还是交易频率有降低，但训练效果特别差，无法接受？
- 如果交易频率有下降，降低了多少？（大概数字有印象吗？））
- 如果训练效果差，有多差呢？
